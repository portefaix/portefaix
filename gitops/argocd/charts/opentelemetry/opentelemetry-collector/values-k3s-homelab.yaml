# Copyright (C) Nicolas Lamirault <nicolas.lamirault@gmail.com>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
# mode: deployment
mode: statefulset

ports:
  - name: metrics
    port: 8888
    protocol: TCP
    targetPort: 8888

envFrom:
  - secretRef:
      name: opentelemetry-datadog-credentials
  - secretRef:
      name: opentelemetry-lightstep-credentials

resources:
  limits:
    # cpu: 500m
    memory: 3Gi
  requests:
    cpu: 500m
    memory: 1Gi

serviceMonitor:
  enabled: true
  extraLabels:
    monitoring: portefaix

# targetAllocator:
#   replicas: 1

collectors:
  - name: metrics
    enabled: true
    image:
      repository: otel/opentelemetry-collector-contrib
      #datasource=github-tags depName=otel/opentelemetry-collector-contrib
      tag: 0.64.0
    serviceMonitor:
      enabled: true
      additionalLabels:
        monitoring: portefaix

    targetAllocator:
      enabled: true
      image:
        repository: ghcr.io/open-telemetry/opentelemetry-operator/target-allocator
        tag: 0.60.0
      replicas: 1
      prometheusCR:
        enabled: true
      # No need for a scrape config when using prometheusCRs
      # scrape_configs_file: scrape_configs_statefulset.yaml

      # TargetAllocator scrapes all ServiceMonitor
      # Not scraped by Prometheus
      serviceMonitor:
        enabled: false
        additionalLabels:
          monitoring: portefaix
    config:
      receivers:

        hostmetrics:
          collection_interval: 60s
          scrapers:
            cpu:
            load:
            memory:
            disk:
            filesystem:
            network:
            processes:

        prometheus:
          config:
            global:
              scrape_interval: 60s
              scrape_timeout: 10s
              evaluation_interval: 30s
              # external_labels:
              #   project: portefaix-homelab
          target_allocator:
            endpoint: http://metrics-targetallocator:80
            interval: 30s
            collector_id: ${POD_NAME}
            http_sd_config:
              refresh_interval: 60s

        # k8s_cluster:
        #   collection_interval: 60s
        #   distribution: kubernetes
        #   node_conditions_to_report: [Ready, DiskPressure, MemoryPressure, PIDPressure, NetworkUnavailable]
        #   allocatable_types_to_report: [cpu, memory, ephemeral-storage, storage]

        # k8s_events:
        #   auth_type: "serviceAccount"

        # kubeletstats:
        #   collection_interval: 60s
        #   auth_type: "serviceAccount"
        #   endpoint: "${K8S_NODE_NAME}:10250"
        #   insecure_skip_verify: true

      processors:

        batch:
          send_batch_max_size: 1000
          timeout: 15s
          send_batch_size : 800

        # Data sources: traces, metrics, logs
        memory_limiter:
          limit_percentage: 90
          spike_limit_percentage: 30
          check_interval: 5s

        # metricstransform:
        #   transforms:
        #      include: .+
        #      match_type: regexp
        #      action: update
        #      operations:
        #        - action: add_label
        #          new_label: kubernetes.cluster.id
        #          new_value: kind-local
        #        - action: add_label
        #          new_label: kubernetes.name
        #          new_value: local

        # k8sattributes:
        #   auth_type: serviceAccount
        #   passthrough: false
        #   filter:
        #     node_from_env_var: K8S_NODE_NAME
        #   extract:
        #     metadata:
        #       - k8s.pod.name
        #       - k8s.pod.uid
        #       - k8s.deployment.name
        #       - k8s.cluster.name
        #       - k8s.namespace.name
        #       - k8s.node.name
        #       - k8s.pod.start_time
        #   pod_association:
        #     - from: resource_attribute
        #       name: k8s.pod.uid

        resource:
          attributes:
          # - key: job
          #   from_attribute: service.name
          #   action: insert
          # - key: service.name
          #   action: upsert
          #   from_attribute: k8s.daemonset.name
          # - key: service.name
          #   action: upsert
          #   from_attribute: k8s.replicaset.name
          # - key: service.name
          #   action: upsert
          #   from_attribute: k8s.statefulset.name
          # - key: service.name
          #   action: upsert
          #   from_attribute: k8s.job.name
          # - key: service.name
          #   action: upsert
          #   from_attribute: k8s.cronjob.name
          - key: collector.name
            value: "${KUBE_POD_NAME}"
            action: insert

        # The resource detection processor adds context related to the cloud provider the Collector is running on.
        # It is necessary **only** on gateway deployment mode, to correctly identify the host that telemetry data comes from.
        # resourcedetection:
        #   detectors: [gcp, ecs, ec2, azure, system]

      exporters:

        logging:
          loglevel: info

        prometheus:
          endpoint: "0.0.0.0:9090"
          metric_expiration: 180m
          # enable_open_metrics: true
          resource_to_telemetry_conversion:
            enabled: true

        # prometheusremotewrite/mimir:
        #   endpoint: "http://mimir-nginx.monitoring.svc.cluster.local:80/api/v1/push"

        # otlp/honeycomb_metrics:
        #   endpoint: "api.honeycomb.io:443"
        #   headers:
        #     "x-honeycomb-team": "${HONEYCOMB_API_KEY}"
        #     "x-honeycomb-dataset": "portefaix-homelab-metrics"

        # otlp/lightstep:
        #   endpoint: ingest.lightstep.com:443
        #   headers:
        #     "lightstep-access-token": "${LIGHTSTEP_TOKEN}"

        # datadog:
        #   # env: prod
        #   # service: opentelemetry
        #   # tags:
        #   #   - cloud:homelab
        #   api:
        #     key: ${DATADOG_API_KEY}
        #     site: datadoghq.com

      extensions:

        health_check:

        memory_ballast:
          size_in_percentage: 20

        # k8s_observer:
        #   auth_type: serviceAccount
        #   node: ${K8S_NODE_NAME}
        #   observe_pods: true
        #   observe_nodes: true

        pprof:
          endpoint: :1888

        zpages:
          endpoint: :55679

      service:

        telemetry:
          logs:
            level: info
          metrics:
            level: detailed
            address: 0.0.0.0:8888

        extensions:
          - health_check
          - memory_ballast
          # - k8s_observer
          - pprof
          - zpages

        pipelines:
          metrics:
            receivers:
              - hostmetrics
              - prometheus
              # - k8s_cluster
              # - kubeletstats
              # - receiver_creator
            processors:
              # - memory_limiter
              # - metricstransform
              # - k8sattributes
              # - resourcedetection/gce
              - batch
            exporters:
              - logging
              - prometheus
              # - prometheusremotewrite/mimir
              # - otlp/honeycomb_metrics
              # - otlp/lightstep
              # - datadog


  - name: traces
    enabled: true
    image:
      repository: otel/opentelemetry-collector-contrib
      #datasource=github-tags depName=otel/opentelemetry-collector-contrib
      tag: 0.64.0
    serviceMonitor:
      enabled: true
      additionalLabels:
        monitoring: portefaix

    targetAllocator:
      enabled: true
      image:
        repository: ghcr.io/open-telemetry/opentelemetry-operator/target-allocator
        tag: 0.60.0
      replicas: 1
      prometheusCR:
        enabled: true
      # No need for a scrape config when using prometheusCRs
      # scrape_configs_file: scrape_configs_statefulset.yaml

      # TargetAllocator scrapes all ServiceMonitor
      # Not scraped by Prometheus
      serviceMonitor:
        enabled: false
        additionalLabels:
          monitoring: portefaix
    config:
      receivers:

        jaeger:
          protocols:
            grpc:
              endpoint: 0.0.0.0:14250
            thrift_http:
              endpoint: 0.0.0.0:14268
            thrift_compact:
              endpoint: 0.0.0.0:6831

        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318

        # zipkin:
        #   endpoint: 0.0.0.0:9411

      processors:

        batch:
          send_batch_max_size: 1000
          timeout: 15s
          send_batch_size : 800

        # Data sources: traces, metrics, logs
        memory_limiter:
          limit_percentage: 90
          spike_limit_percentage: 30
          check_interval: 5s

        resource:
          attributes:
          # - key: job
          #   from_attribute: service.name
          #   action: insert
          # - key: service.name
          #   action: upsert
          #   from_attribute: k8s.daemonset.name
          # - key: service.name
          #   action: upsert
          #   from_attribute: k8s.replicaset.name
          # - key: service.name
          #   action: upsert
          #   from_attribute: k8s.statefulset.name
          # - key: service.name
          #   action: upsert
          #   from_attribute: k8s.job.name
          # - key: service.name
          #   action: upsert
          #   from_attribute: k8s.cronjob.name
          - key: collector.name
            value: "${KUBE_POD_NAME}"
            action: insert

        # The resource detection processor adds context related to the cloud provider the Collector is running on.
        # It is necessary **only** on gateway deployment mode, to correctly identify the host that telemetry data comes from.
        # resourcedetection:
        #   detectors: [gcp, ecs, ec2, azure, system]

      exporters:

        logging:
          loglevel: info

        # Data sources: traces, metrics, logs
        otlp/tempo:
          endpoint: tempo.tracing.svc.cluster.local:4317
          tls:
            insecure_skip_verify: true
            insecure: true

        # otlp/honeycomb_metrics:
        #   endpoint: "api.honeycomb.io:443"
        #   headers:
        #     "x-honeycomb-team": "${HONEYCOMB_API_KEY}"
        #     "x-honeycomb-dataset": "portefaix-homelab-metrics"

        # otlp/lightstep:
        #   endpoint: ingest.lightstep.com:443
        #   headers:
        #     "lightstep-access-token": "${LIGHTSTEP_TOKEN}"

        # datadog:
        #   # env: prod
        #   # service: opentelemetry
        #   # tags:
        #   #   - cloud:homelab
        #   api:
        #     key: ${DATADOG_API_KEY}
        #     site: datadoghq.com

      extensions:

        health_check:

        memory_ballast:
          size_in_percentage: 20

        # k8s_observer:
        #   auth_type: serviceAccount
        #   node: ${K8S_NODE_NAME}
        #   observe_pods: true
        #   observe_nodes: true

        pprof:
          endpoint: :1888

        zpages:
          endpoint: :55679

      service:

        telemetry:
          logs:
            level: info
          metrics:
            level: detailed
            address: 0.0.0.0:8888

        extensions:
          - health_check
          - memory_ballast
          # - k8s_observer
          - pprof
          - zpages

        pipelines:

          traces:
            receivers:
              - otlp
            processors:
              - batch
              # - k8sattributes
              - memory_limiter
              - batch
              - resource
            exporters:
              - logging
              # - otlp/tempo
              # - otlp/honeycomb_traces
              # - otlp/lightstep
              # - datadog

  - name: logs
    enabled: true
    image:
      repository: otel/opentelemetry-collector-contrib
      #datasource=github-tags depName=otel/opentelemetry-collector-contrib
      tag: 0.64.0
    serviceMonitor:
      enabled: true
      additionalLabels:
        monitoring: portefaix

    targetAllocator:
      enabled: true
      image:
        repository: ghcr.io/open-telemetry/opentelemetry-operator/target-allocator
        tag: 0.60.0
      replicas: 1
      prometheusCR:
        enabled: true
      # No need for a scrape config when using prometheusCRs
      # scrape_configs_file: scrape_configs_statefulset.yaml

      # TargetAllocator scrapes all ServiceMonitor
      # Not scraped by Prometheus
      serviceMonitor:
        enabled: false
        additionalLabels:
          monitoring: portefaix

    config:

      receivers:
        otlp:
          protocols:
            grpc:
            http:
        fluentforward:
          endpoint: 0.0.0.0:24224

      processors:

        attributes:
          actions:
          - action: insert
            key: loki.attribute.labels
            value: http.status_code

        resource:
          attributes:
          - action: insert
            key: loki.attribute.labels
            value: http.status
          - action: insert
            key: loki.resource.labels
            value: host.name, pod.name

        batch:
          send_batch_max_size: 1000
          timeout: 15s
          send_batch_size : 800

        # Data sources: traces, metrics, logs
        memory_limiter:
          limit_percentage: 90
          spike_limit_percentage: 30
          check_interval: 5s

        # The resource detection processor adds context related to the cloud provider the Collector is running on.
        # It is necessary **only** on gateway deployment mode, to correctly identify the host that telemetry data comes from.
        # resourcedetection:
        #   detectors: [gcp, ecs, ec2, azure, system]

      exporters:

        logging:
          loglevel: info

        # https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/lokiexporter/README.md
        loki:
          endpoint: http://loki-gateway.logging.svc.cluster.local:80/loki/api/v1/push
          tenant_id: homelab
          labels:
            resource:
              # Allowing 'container.name' attribute and transform it to 'container_name', which is a valid Loki label name.
              container.name: "container_name"
              k8s.cluster.name: "k8s_cluster_name"
              k8s.event.reason: "k8s_event_reason"
              k8s.object.kind: "k8s_object_kind"
              k8s.object.name: "k8s_object_name"
              k8s.object.uid: "k8s_object_uid"
              k8s.object.fieldpath: "k8s_object_fieldpath"
              k8s.object.api_version: "k8s_object_api_version"
            attributes:
              k8s.event.reason: "k8s_event_reason"
              k8s.event.action: "k8s_event_action"
              k8s.event.start_time: "k8s_event_start_time"
              k8s.event.name: "k8s_event_name"
              k8s.event.uid: "k8s_event_uid"
              k8s.namespace.name: "k8s_namespace_name"
              k8s.event.count: "k8s_event_count"
            record:
              # Adds 'traceID' as a log label, seen as 'traceid' in Loki.
              traceID: "traceid"
          # headers:
          #   "X-Custom-Header": "portefaix_homelab"

        # otlp/lightstep:
        #   endpoint: ingest.lightstep.com:443
        #   headers:
        #     "lightstep-access-token": "${LIGHTSTEP_TOKEN}"

        # datadog:
        #   # env: prod
        #   # service: opentelemetry
        #   # tags:
        #   #   - cloud:homelab
        #   api:
        #     key: ${DATADOG_API_KEY}
        #     site: datadoghq.com

      extensions:

        health_check:

        memory_ballast:
          size_in_percentage: 20

        # k8s_observer:
        #   auth_type: serviceAccount
        #   node: ${K8S_NODE_NAME}
        #   observe_pods: true
        #   observe_nodes: true

        pprof:
          endpoint: :1888

        zpages:
          endpoint: :55679

      service:

        telemetry:
          logs:
            level: info
          metrics:
            level: detailed
            address: 0.0.0.0:8888

        extensions:
          - health_check
          - memory_ballast
          # - k8s_observer
          - pprof
          - zpages

        pipelines:

          logs:
            receivers:
              - fluentforward
              # - otlp
            processors:
              - resource
              - attributes
              - batch
              - memory_limiter
            exporters:
              - logging
              # - loki
