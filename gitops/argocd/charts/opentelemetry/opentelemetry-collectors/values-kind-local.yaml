# Copyright (C) Nicolas Lamirault <nicolas.lamirault@gmail.com>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
mode: deployment
# TODO: https://github.com/open-telemetry/opentelemetry-operator/pull/1049
mode: "statefulset"

ports:
  - name: metric
    port: 9090
    protocol: TCP
    targetPort: 9090

resources:
  limits:
    # cpu: 500m
    memory: 1Gi
  requests:
    cpu: 100m
    memory: 512Mi

serviceMonitor:
  enabled: true
  extraLabels:
    monitoring: portefaix

config:
  receivers:

    hostmetrics:
      collection_interval: 60s
      scrapers:
        cpu: {}
        load: {}
        memory: {}
        disk: {}
        filesystem: {}
        network: {}

    jaeger:
      protocols:
        grpc:
          endpoint: 0.0.0.0:14250
        thrift_http:
          endpoint: 0.0.0.0:14268
        thrift_compact:
          endpoint: 0.0.0.0:6831

    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318

    prometheus:
      config:
        global:
          scrape_interval: 60s
          scrape_timeout: 10s
          evaluation_interval: 30s
          external_labels:
            cluster: portefaix-local
        # TODO: https://github.com/open-telemetry/opentelemetry-operator/pull/1049
        # Revove scrape_configs and add:
        # target_allocator:
        scrape_configs:
          - job_name: serviceMonitor/monitoring/argocd-metrics/0
          - job_name: serviceMonitor/monitoring/argocd-repo-server-metrics/0
          - job_name: serviceMonitor/monitoring/argocd-server-metrics/0
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-alertmanager/0
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-apiserver/0
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-coredns/0
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-kube-controller-manager/0
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-kube-dns/0
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-kube-dns/1
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-kube-etcd/0
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-kube-proxy/0
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-kubelet/0
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-kubelet/1
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-kubelet/2
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-operator/0
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-prometheus/0
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-stack-kube-state-metrics/0
          - job_name: serviceMonitor/monitoring/portefaix-kube-prometheus-stack-prometheus-node-exporter/0
          - job_name: serviceMonitor/monitoring/portefaix-grafana/0


    # zipkin:
    #   endpoint: 0.0.0.0:9411

    # k8s_cluster:
    #   collection_interval: 60s
    #   distribution: kubernetes
    #   node_conditions_to_report: [Ready, DiskPressure, MemoryPressure, PIDPressure, NetworkUnavailable]
    #   allocatable_types_to_report: [cpu, memory, ephemeral-storage, storage]

    # k8s_events:
    #   auth_type: "serviceAccount"

    # kubeletstats:
    #   collection_interval: 60s
    #   auth_type: "serviceAccount"
    #   endpoint: "${K8S_NODE_NAME}:10250"
    #   insecure_skip_verify: true

    # receiver_creator:
    #    watch_observers: [k8s_observer]
    #    receivers:
    #       kubeletstats:
    #         rule: type == "k8s.node"
    #         config:
    #           collection_interval: 60s
    #           auth_type: "serviceAccount"
    #           # endpoint: "https://${K8S_NODE_NAME}:10250"
    #           # auth_type: "none"
    #           # endpoint: "http://${K8S_NODE_NAME}:10255"
    #           # insecure_skip_verify: true
    #           endpoint: "`endpoint`:`kubelet_endpoint_port`"
    #           insecure_skip_verify: true
    #           extra_metadata_labels:
    #             - container.id
    #             - k8s.volume.type
    #           metric_groups:
    #             - node
    #             - pod
    #             - volume
    #             - container

  processors:

    batch:
      send_batch_max_size: 1000
      timeout: 30s
      send_batch_size : 800

    # Data sources: traces, metrics, logs
    memory_limiter:
      limit_percentage: 90
      spike_limit_percentage: 30
      check_interval: 5s

    # metricstransform:
    #   transforms:
    #      include: .+
    #      match_type: regexp
    #      action: update
    #      operations:
    #        - action: add_label
    #          new_label: kubernetes.cluster.id
    #          new_value: kind-local
    #        - action: add_label
    #          new_label: kubernetes.name
    #          new_value: local

    # k8sattributes:
    #   auth_type: serviceAccount
    #   passthrough: false
    #   filter:
    #     node_from_env_var: K8S_NODE_NAME
    #   extract:
    #     metadata:
    #       - k8s.pod.name
    #       - k8s.pod.uid
    #       - k8s.deployment.name
    #       - k8s.cluster.name
    #       - k8s.namespace.name
    #       - k8s.node.name
    #       - k8s.pod.start_time
    #   pod_association:
    #     - from: resource_attribute
    #       name: k8s.pod.uid

  exporters:

    logging:
      loglevel: info

    prometheus:
      endpoint: "0.0.0.0:9090"
      metric_expiration: 180m
      resource_to_telemetry_conversion:
        enabled: true

    # Data sources: metrics
    # prometheusremotewrite/mimir:
    #   endpoint: "http://mimir.monitoring.svc.cluster.local:9411/api/prom/push"

    # Data sources: logs
    # https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/lokiexporter/README.md
    loki:
      endpoint: "http://loki.logging.svc.cluster.local:3100/loki/api/v1/push"
      tenant_id: "local"
      labels:
        resource:
          # Allowing 'container.name' attribute and transform it to 'container_name', which is a valid Loki label name.
          container.name: "container_name"
          k8s.cluster.name: "k8s_cluster_name"
          k8s.event.reason: "k8s_event_reason"
          k8s.object.kind: "k8s_object_kind"
          k8s.object.name: "k8s_object_name"
          k8s.object.uid: "k8s_object_uid"
          k8s.object.fieldpath: "k8s_object_fieldpath"
          k8s.object.api_version: "k8s_object_api_version"
        attributes:
          k8s.event.reason: "k8s_event_reason"
          k8s.event.action: "k8s_event_action"
          k8s.event.start_time: "k8s_event_start_time"
          k8s.event.name: "k8s_event_name"
          k8s.event.uid: "k8s_event_uid"
          k8s.namespace.name: "k8s_namespace_name"
          k8s.event.count: "k8s_event_count"
        record:
          # Adds 'traceID' as a log label, seen as 'traceid' in Loki.
          traceID: "traceid"
      # headers:
      #   "X-Custom-Header": "portefaix_homelab"

    # Data sources: traces, metrics, logs
    otlp/tempo:
      endpoint: tempo.tracing.svc.cluster.local:4317
      tls:
        insecure_skip_verify: true
        insecure: true

    # Data sources: traces, metrics, logs
    # otlp/honeycomb:
    #   endpoint: "api.honeycomb.io:443"
    #   headers:
    #     "x-honeycomb-team": "${HONEYCOMB_API_KEY}"
    #     "x-honeycomb-dataset": "portefaix-homelab" # for Metrics

    # Data sources: traces, metrics, logs
    # otlp/aspecto:
    #   endpoint: otelcol.aspecto.io:4317
    #   headers:
    #     Authorization: ${ASPECTO_API_KEY}

    # datadog:
    #   env: prod
    #   service: opentelemetry
    #   tags:
    #     - cloud:homelab
    #   api:
    #     key: ${DATADOG_API_KEY}
    #     site: datadoghq.eu

    # mezmo:
    #   ingest_url: "https://logs.logdna.com/log/ingest"
    #   ingest_key: "${MEZMO_API_KEY}"

  extensions:

    health_check:

    memory_ballast:
      size_in_percentage: 20

    # k8s_observer:
    #   auth_type: serviceAccount
    #   node: ${K8S_NODE_NAME}
    #   observe_pods: true
    #   observe_nodes: true

    pprof:
      endpoint: :1888

    zpages:
      endpoint: :55679

  service:

    telemetry:
      metrics:
        address: 0.0.0.0:8888

    extensions:
      - health_check
      - memory_ballast
      # - k8s_observer
      - pprof
      - zpages

    pipelines:

      logs:
        receivers:
          - otlp
          # - k8s_events
        processors:
          - batch
          # - k8sattributes
          - memory_limiter
        exporters:
          - logging
          # - loki

      metrics:
        receivers:
          - hostmetrics
          - prometheus
          # - k8s_cluster
          # - kubeletstats
          # - receiver_creator
          - otlp
        processors:
          # - memory_limiter
          # - metricstransform
          # - k8sattributes
          # - resourcedetection/gce
          - batch
        exporters:
          - logging
          - prometheus
          # - prometheusremotewrite/mimir
          # - datadog

      traces:
        receivers:
          - otlp
        processors:
          - batch
          # - k8sattributes
          - memory_limiter
        exporters:
          - logging
          # - otlp/tempo
